{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "## Assignment 3 on Natural Language Processing\n",
        "\n",
        "## Date : 30th Sept, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsJhcdwraFLG"
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      },
      "source": [
        "Please submit with outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElRkQElWUMjG"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOSbZnhkaFLQ",
        "outputId": "a833a69f-1f0a-4369-f322-c290dc330acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhHRim2AUm4z",
        "outputId": "7b2d7257-e660-4276-fa58-61a34e1aa3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
        "dataset = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK_Hn2f6VMP7"
      },
      "source": [
        "# Preprocessing\n",
        "PrePrecessing that needs to be done on lower cased corpus\n",
        "\n",
        "1. Remove html tags\n",
        "2. Remove URLS\n",
        "3. Remove non alphanumeric character\n",
        "4. Remove Stopwords\n",
        "5. Perform stemming and lemmatization\n",
        "\n",
        "You can use regex from re. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B5lHZPsVOXv",
        "outputId": "6754f443-6499-4984-f521-a28e16590692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "#all text to lowercase and remove unwanted signs \n",
        "def remove_unwanted_chars(text):\n",
        "    allowed_chars = \"\"\" abcdefghijklmnopqrstuvwxyz;,!.?\"\"\"\n",
        "    clean_text = text.lower()\n",
        "    for c in clean_text:\n",
        "        if allowed_chars.find(c) == -1:\n",
        "            clean_text = clean_text.replace(c, \"\")\n",
        "        else:\n",
        "            pass\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_html(text):    #removing HTML\n",
        "    html = re.compile(r\"<.*?>\")\n",
        "    return html.sub(r\"\", text)\n",
        "\n",
        "\n",
        "def remove_URL(text):  #removing URL\n",
        "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    return url.sub(r\"\", text)\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE,\n",
        "    )\n",
        "    return emoji_pattern.sub(r\"\", text)\n",
        "\n",
        "\n",
        "def remove_stopwords(text, stop_words):  #removing stopwords\n",
        "    filtered_sentence = []\n",
        "    for w in text.split(\" \"): \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)\n",
        "    text = \" \".join(filtered_sentence)\n",
        "    return text\n",
        "\n",
        "\n",
        "def make_sentences_correct(text): #getting correct words\n",
        "    val = re.split('[!.,?;\\s]+', text)\n",
        "    text = \" \".join(val)\n",
        "    return text             \n",
        "\n",
        "def get_lemma(text): # getting lemmas\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return \" \".join(word_tokens)\n",
        "\n",
        "                        \n",
        "def remove_white_spaces(text): #removing whitespaces from start and end\n",
        "    text = text.strip() \n",
        "    return text      \n",
        "\n",
        "\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: remove_unwanted_chars(x))\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: remove_URL(x))\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: remove_html(x))\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: remove_emoji(x))\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: remove_stopwords(x, stop_words))\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: make_sentences_correct(x))\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: get_lemma(x))\n",
        "dataset[\"review\"] = dataset.review.map(lambda x: remove_white_spaces(x))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonderful little production br br filming tech...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically theres family little boy jake thinks...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter matteis love time money visually stunni...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>thought movie right good job wasnt creative or...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>catholic taught parochial elementary schools n...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>im going disagree previous comment side maltin...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>one expects star trek movies high art fans exp...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      one reviewers mentioned watching oz episode yo...  positive\n",
              "1      wonderful little production br br filming tech...  positive\n",
              "2      thought wonderful way spend time hot summer we...  positive\n",
              "3      basically theres family little boy jake thinks...  negative\n",
              "4      petter matteis love time money visually stunni...  positive\n",
              "...                                                  ...       ...\n",
              "49995  thought movie right good job wasnt creative or...  positive\n",
              "49996  bad plot bad dialogue bad acting idiotic direc...  negative\n",
              "49997  catholic taught parochial elementary schools n...  negative\n",
              "49998  im going disagree previous comment side maltin...  negative\n",
              "49999  one expects star trek movies high art fans exp...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaSkfcvYGXk",
        "outputId": "8e640c29-c45e-426e-a7a8-2219b367a31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
        "print(\"Distribution of two classes: \")\n",
        "print(dataset.groupby(\"sentiment\").sentiment.count())\n",
        "\n",
        "count = 0\n",
        "word_count = []\n",
        "for i in range(50000):\n",
        "    temp = len(dataset.iloc[i][\"review\"].split(\" \"))\n",
        "    word_count.append(temp)\n",
        "    count+=temp\n",
        "\n",
        "print(\"Average number of words in each document: \", count/dataset.shape[0])\n",
        "print(\"Plot showing number of words variation in documents: \")\n",
        "plt.bar(np.arange(50000), word_count, width=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution of two classes: \n",
            "sentiment\n",
            "negative    25000\n",
            "positive    25000\n",
            "Name: sentiment, dtype: int64\n",
            "Average number of words in each document:  126.7472\n",
            "Plot showing number of words variation in documents: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 50000 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FkJ-e2pUwun"
      },
      "source": [
        "# Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVq-mN28U_J4"
      },
      "source": [
        "# get reviews column from df\n",
        "reviews = dataset[\"review\"]\n",
        "\n",
        "# get labels column from df\n",
        "labels = dataset[\"sentiment\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljo5NquhXTXr"
      },
      "source": [
        "# Use label encoder to encode labels. Convert to 0/1\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzG-C_EVWWET",
        "outputId": "b9b4c89a-f9d7-453a-e17d-14d74d46fcdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "# Split the data into train and test (80% - 20%). \n",
        "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, encoded_labels, test_size=0.2, random_state=0, stratify=encoded_labels)\n",
        "print(\"length of training dataset:\", len(X_train.index))\n",
        "print(\"length of testing dataset:\", len(X_test.index))\n",
        "print(\"*******First sentence in train dataset*********\")\n",
        "print(X_train[0])\n",
        "print(\"*******First sentence class in test dataset*********\")\n",
        "print(y_train[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of training dataset: 40000\n",
            "length of testing dataset: 10000\n",
            "*******First sentence in train dataset*********\n",
            "one reviewers mentioned watching oz episode youll hooked right exactly happened me br br first thing struck oz brutality unflinching scenes violence set right word go trust me show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word br br called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many aryans muslims gangstas latinos christians italians irish more so scuffles death stares dodgy dealings shady agreements never far away br br would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romance oz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready it watched more developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away it well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side\n",
            "*******First sentence class in test dataset*********\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImxNuMs7aFLh",
        "outputId": "09b8fcf9-2cf4-453d-ceb3-80d060f6b763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000,)\n",
            "(10000,)\n",
            "(40000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz1YdsSkiWCX"
      },
      "source": [
        "Here there are two approaches possible for building vocabulary for the naive Bayes.\n",
        "1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
        "2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
        " \n",
        "You are supposed to go by the 2nd approach.\n",
        " \n",
        "Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
        "\n",
        "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
        "\n",
        "\n",
        "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
        "\n",
        "$N_{w_j}$ : Total count of features in class $w_j$\n",
        "\n",
        "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
        "\n",
        "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cllNfGmUr77",
        "outputId": "d582195b-78c2-40ec-8f8d-ba8687ff2469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#term document matrix for class 1\n",
        "docs_1 = [X_train[i] for i in X_train.index if encoded_labels[i] == 1]\n",
        "vec_1 = CountVectorizer(max_features=3000)\n",
        "X_1 = vec_1.fit_transform(docs_1)\n",
        "\n",
        "#term document matrix for class 0\n",
        "docs_0 = [X_train[i] for i in X_train.index if encoded_labels[i] == 0]\n",
        "vec_0 = CountVectorizer(max_features=3000)\n",
        "X_0 = vec_0.fit_transform(docs_0)\n",
        "\n",
        "#frequency of each word in class 1\n",
        "word_list_1 = vec_1.get_feature_names();    \n",
        "count_list_1 = X_1.toarray().sum(axis=0) \n",
        "freq_1 = dict(zip(word_list_1,count_list_1))\n",
        "\n",
        "#frequency of each word in class 0\n",
        "word_list_0 = vec_0.get_feature_names();    \n",
        "count_list_0 = X_0.toarray().sum(axis=0) \n",
        "freq_0 = dict(zip(word_list_0,count_list_0))\n",
        "\n",
        "#total count of features in class 1\n",
        "total_cnts_features_1 = count_list_1.sum(axis=0)\n",
        "print(total_cnts_features_1)\n",
        "\n",
        "#total count of features in class 0\n",
        "total_cnts_features_0 = count_list_0.sum(axis=0)\n",
        "print(total_cnts_features_0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1896932\n",
            "1916509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzRvPjWaWUnm"
      },
      "source": [
        "# predicting the probalility of words using laplace smoothing for class 1\n",
        "prob_1_with_ls = []\n",
        "for sent in X_test:\n",
        "    prob = 1\n",
        "    for word in sent.split():\n",
        "        if word in freq_1.keys():\n",
        "            count = freq_1[word]\n",
        "        else:\n",
        "            count = 0\n",
        "        prob *= (count + 1)/(total_cnts_features_1 + 3000)\n",
        "    prob_1_with_ls.append(prob)\n",
        "    \n",
        "# predicting the probalility of words using laplace smoothing for class 0\n",
        "prob_0_with_ls = []\n",
        "for sent in X_test:\n",
        "    prob = 1\n",
        "    for word in sent.split():\n",
        "        if word in freq_0.keys():\n",
        "            count = freq_0[word]\n",
        "        else:\n",
        "            count = 0\n",
        "        prob *= (count + 1)/(total_cnts_features_0 + 3000)\n",
        "    prob_0_with_ls.append(prob)\n",
        "    \n",
        "\n",
        "# predicting the classes of sentences in the test data\n",
        "y_test_pred = []\n",
        "for i in range(len(prob_0_with_ls)):\n",
        "    if prob_0_with_ls[i]>prob_1_with_ls[i]:\n",
        "        y_test_pred.append(0)\n",
        "    else:\n",
        "        y_test_pred.append(1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtQSl1zvW4DD",
        "outputId": "8bde9d21-d90f-47a4-bdc3-65e820aa9c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Test the model on test set and report Accuracy\n",
        "correct = 0\n",
        "for i in range(len(y_test_pred)):\n",
        "    if y_test[i] == y_test_pred[i]:\n",
        "        correct+=1\n",
        "print(\"Obtained accuracy using Naive Bayes Classifier is:\", (correct*100)/len(y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtained accuracy using Naive Bayes Classifier is: 63.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNql0acU7sa"
      },
      "source": [
        "# *LSTM* based Classifier\n",
        "\n",
        "Use the above train and test splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkqnvbUOXoN0"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = 3000\n",
        "oov_tok = '<OOK>'\n",
        "embedding_dim = 100\n",
        "max_length = 180\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeycEg9nZAOF"
      },
      "source": [
        "# splitting the data \n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, test_size=0.2, random_state=0, stratify=encoded_labels)\n",
        "\n",
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# convert train dataset to sequence and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# convert Test dataset to sequence and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtw3w895ZP39",
        "outputId": "faf79127-119b-4293-8f9c-f710f58c382f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "# model initialization\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 180, 100)          300000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                3096      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 387,601\n",
            "Trainable params: 387,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skmaDJMnZTzc",
        "outputId": "84b734fc-5f9e-44ab-f055-b1dec1062240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(train_padded, train_labels, epochs=num_epochs, verbose=1, validation_split=0.1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36000 samples, validate on 4000 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f4624206d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f4624206d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "36000/36000 [==============================] - 261s 7ms/sample - loss: 0.3855 - accuracy: 0.8269 - val_loss: 0.3128 - val_accuracy: 0.8755\n",
            "Epoch 2/5\n",
            "36000/36000 [==============================] - 261s 7ms/sample - loss: 0.2837 - accuracy: 0.8856 - val_loss: 0.3031 - val_accuracy: 0.8723\n",
            "Epoch 3/5\n",
            "36000/36000 [==============================] - 261s 7ms/sample - loss: 0.2483 - accuracy: 0.9019 - val_loss: 0.3089 - val_accuracy: 0.8730\n",
            "Epoch 4/5\n",
            "36000/36000 [==============================] - 264s 7ms/sample - loss: 0.2186 - accuracy: 0.9161 - val_loss: 0.3145 - val_accuracy: 0.8725\n",
            "Epoch 5/5\n",
            "36000/36000 [==============================] - 261s 7ms/sample - loss: 0.2059 - accuracy: 0.9201 - val_loss: 0.3397 - val_accuracy: 0.8560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEhWEr5Zq7M",
        "outputId": "c6939aa2-1987-4cdc-ebdf-60c93a3842cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Calculate accuracy on Test data\n",
        "prediction = model.predict(test_padded)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "predicted_test = []\n",
        "for item in prediction:\n",
        "  if item >= 0.5:\n",
        "    predicted_test.append(1)\n",
        "  else:\n",
        "    predicted_test.append(0)\n",
        "\n",
        "# Accuracy : one can use classification_report from sklearn\n",
        "correct = 0\n",
        "for i in range(len(predicted_test)):\n",
        "  if predicted_test[i] == test_labels[i]:\n",
        "    correct+=1\n",
        "\n",
        "print(\"Accuracy of the model on the test data is:\", (correct*100)/len(predicted_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test data is: 85.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIICV-ySOYL0"
      },
      "source": [
        "## Get predictions for random examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2RmfNL3OYL0",
        "outputId": "b4667dc2-f693-43d5-d0db-e93b1a605c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# reviews on which we need to predict\n",
        "sentence = [\"The movie was very touching and heart whelming\", \n",
        "            \"I have never seen a terrible movie like this\", \n",
        "            \"the movie plot is terrible but it had good acting\"]\n",
        "\n",
        "# convert to a sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "# pad the sequence\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# Get probabilities\n",
        "print(model.predict(padded))\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "\n",
        "predicted_examples = []\n",
        "for item in model.predict(padded):\n",
        "  if item >= 0.5:\n",
        "    predicted_examples.append(1)\n",
        "  else:\n",
        "    predicted_examples.append(0)\n",
        "\n",
        "print(predicted_examples)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.79820657]\n",
            " [0.22194049]\n",
            " [0.05066691]]\n",
            "[1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLdP6lRwS5hq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}